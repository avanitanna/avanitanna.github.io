<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://avanitanna.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://avanitanna.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-17T03:39:44+00:00</updated><id>https://avanitanna.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Recipe Finder Agent using smolagents</title><link href="https://avanitanna.github.io/blog/2025/hf-recipe-agent/" rel="alternate" type="text/html" title="Recipe Finder Agent using smolagents"/><published>2025-02-16T00:00:00+00:00</published><updated>2025-02-16T00:00:00+00:00</updated><id>https://avanitanna.github.io/blog/2025/hf-recipe-agent</id><content type="html" xml:base="https://avanitanna.github.io/blog/2025/hf-recipe-agent/"><![CDATA[<p>As an AI enthusiast and home cook, I built a Recipe Finder Agent using Hugging Face’s smolagents framework. This agent is designed to solve a common problem: figuring out what to cook with the ingredients you already have, while respecting dietary restrictions and cuisine preferences. The agent articularly focus on the crucial “thought-action-observation” cycle that drives its behavior. <a href="https://github.com/huggingface/smolagents">Smolagents</a> is a library that focuses on <code class="language-plaintext highlighter-rouge">codeAgent</code>, a kind of agent that performs “Actions” through code blocks, and then “Observes” results by executing the code.</p> <h4 id="what-makes-an-agent-agentic">What makes an Agent “Agentic”?</h4> <p>Before diving into implementation details, let’s clarify what we mean by an “agentic workflow.” Unlike traditional software that follows predefined paths, an agentic system:</p> <ul> <li>Perceives its environment through data inputs</li> <li>Thinks and decides on appropriate actions through reasoning</li> <li>Acts using available tools</li> <li>Observes and learns from the results of those actions</li> </ul> <h4 id="project-overview-the-recipe-finder-agent">Project Overview: The Recipe Finder Agent</h4> <p>The Recipe Finder Agent is designed to solve a common household problem: What can I cook with what I have? By combining LLM capabilities with domain-specific tools, this agent helps you discover recipes based on:</p> <ul> <li>Ingredients you have available</li> <li>Dietary restrictions</li> <li>Cuisine type preferences</li> </ul> <p>Thought-Action-Observation cycle: The agent works by leveraging the ReAct approach through which the agent is able to “think” step by step in order to generate a plan before jumping to the final answer. Thoughts represent the Agent’s internal reasoning and planning processes to solve the task and are responsible for accessing current observations and decide what the next action(s) should be. Actions are the concrete steps the AI agent takes to interact with its environment. For example, a code agent writes a code block that is interpreted externally. Observations are how an Agent perceives the consequences of its actions. The information provided by observations feeds back into the agent and improves its thought process and guides future actions.</p> <h4 id="component-architecture-and-implementation">Component Architecture and Implementation</h4> <p>The agent setup combines and consists of several key components:</p> <ul> <li>A language model (Qwen2.5-Coder-32B)</li> <li>Specialized tools for web search (using duckduckgo search), and webpage visiting,</li> <li>A recipe-finding tool that leverages these lower-level tools and considers user’s preferences (e.g., available ingredients, dietary restrictions and cuisine type)</li> <li>Custom prompt templates that define the agent’s reasoning process</li> </ul> <p>At the heart of an effective agent lies its prompting strategy - check out <code class="language-plaintext highlighter-rouge">prompts.yml</code> to look at the critical Thought-Action-Observation cycle.</p> <p>The power of the recipe finder agent comes from the specialized <code class="language-plaintext highlighter-rouge">AdvancedRecipeFinderTool</code> that orchestrates lower-level tools. The recipe finding tool demonstrates a crucial principle: <code class="language-plaintext highlighter-rouge">composability</code>. It leverages lower-level tools (web search and webpage parsing) to create a higher-level capability (recipe finding). This composability is what allows the agent to tackle increasingly complex tasks.</p> <p>Another critical aspect of the agent implementation is the planning process. The prompt templates include specialized sections for initial planning and plan updates. Planning is what:</p> <ul> <li>Forces the agent to think ahead rather than react impulsively</li> <li>Helps the agent decompose complex problems into manageable steps</li> <li>Provides a framework for handling unexpected results by updating the plan</li> </ul> <p>The system also uses a clever fact-tracking mechanism to maintain context.</p> <p>Check out this <a href="https://huggingface.co/spaces/vntnn/First_agent_template/tree/main">repo</a> for implementation details.</p> <h4 id="recommended-how-to-run-the-codeagent-more-securely">[Recommended] How to run the codeAgent (more securely):</h4> <p>Note that you’ll have to set your HF token first in an <code class="language-plaintext highlighter-rouge">.env</code> file if you choose to use dotenv.</p> <p>It is recommended to run a code agent using a docker container, e.g., <code class="language-plaintext highlighter-rouge">docker run -it ubuntu:latest bin/bash</code>.</p> <p>[Warning!] Once you run <code class="language-plaintext highlighter-rouge">python3 app.py</code>, it spawns a public URL that is linked to your container. HF claims that it runs using a secure local Python Interpretor https://smolagents.org/docs/secure-code-execution-of-smolagents/.</p> <h4 id="building-your-own-agentic-workflows">Building your own agentic workflows</h4> <p>Here are some key principles for effective agentic workflows:</p> <ul> <li>Structure the Thought-Action-Observation Cycle The explicit cycle of thinking, acting, and observing is what separates agents from simple LLM calls. Also ensure that your prompts enforce this cycle.</li> <li>Take advantage of Tool Composability Design tools that can be combined in flexible ways. The power of agents comes from composing simple tools to solve complex problems.</li> <li>Implement Explicit Planning Force the agent to create and update plans. This makes the agent more strategic and helps it recover from failures.</li> <li>Track Facts Systematically Implement a system for the agent to track what it knows, what it has learned, and what it still needs to discover. This helps it make better decisions.</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Recipe recommendation]]></summary></entry><entry><title type="html">[Part 1] Unlocking the Power of LLMs for the Cashflow Game</title><link href="https://avanitanna.github.io/blog/2025/cashflow-1/" rel="alternate" type="text/html" title="[Part 1] Unlocking the Power of LLMs for the Cashflow Game"/><published>2025-02-01T00:00:00+00:00</published><updated>2025-02-01T00:00:00+00:00</updated><id>https://avanitanna.github.io/blog/2025/cashflow-1</id><content type="html" xml:base="https://avanitanna.github.io/blog/2025/cashflow-1/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cashflow_game-480.webp 480w,/assets/img/cashflow_game-800.webp 800w,/assets/img/cashflow_game-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/cashflow_game.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As a passionate learners and game enthusiasts, <a href="https://olmeke.github.io/">Marius Fleischer</a> and I recently took on the challenge of designing an interactive “rat race” workflow of the popular <a href="https://www.richdad.com/classic">Cashflow game</a> by Robert Kiyosaki, author of the iconic book <strong>“Rich Dad, Poor Dad</strong>”. Cashflow is a financial education game that teaches players about investment strategies, asset management, and personal finance.</p> <p>The goal was to leverage the power of large language models (LLMs) to build an interactive, AI-driven Cashflow game experience – which can run on an edge device in a cost effective manner. Through this project, I gained valuable insights into the design and implementation of such workflows.</p> <h4 id="initial-design-and-workflow-concepts">Initial Design and Workflow Concepts</h4> <p>Here’s how we started out by brainstorming and building this agentic workflow:</p> <div class="row"> <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cashflow-2-480.webp 480w,/assets/img/cashflow-2-800.webp 800w,/assets/img/cashflow-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/cashflow-2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cashflow-1-480.webp 480w,/assets/img/cashflow-1-800.webp 800w,/assets/img/cashflow-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/cashflow-1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An analyst agent (left). Full agentic workflow for the game (right). </div> <h4 id="game-workflow-breakdown">Game Workflow Breakdown</h4> <p>Initial workflow involved an analyst agent using tools (such as a calculator) to analyze any game decisions. This comes up in many places in the workflow. The workflow for each turn follows the rules/ structure of the game within the “rat race” –</p> <ul> <li>Evaluate shared opportunities (if any) and decide if you want to act on it</li> <li>Decide on bank interactions such as repaying or borrowing</li> <li>Roll the die</li> <li>Resolve various card types - options include deals (small/big, allowing you to buy assets), charity (you can do so if you have the money to, in exchange for more options to roll the die), doodad cards (getting a baby, payday, buying things), and market cards (opportunities that allow you to sell assets)</li> </ul> <p>Analyst agent can analyze decisions and assess your financial situation (managed through a financial statement). It will make a recommendation and the game goes on!</p> <h4 id="challenges-and-insights">Challenges and Insights</h4> <p>Implementing the workflow with Llama 3.2 (via Ollama) revealed critical challenges and design considerations. Caveat: Larger models would have probably done better, but the goal was to develop this solution on edge devices with minimal costs incurred. Attempting to use <a href="https://python.langchain.com/docs/concepts/tool_calling/">tool calling</a> for different card types proved challenging. Direct computation emerged as a more efficient approach. With all the game data (including financial statement) readily available, it became clear that simply presenting information to the LLM could work effectively. Furthermore, not every workflow requires complex agent interactions! This led to reconsidering Agentic vs simple LLM workflow and revisiting the crucial differences between the two.</p> <h4 id="agentic-vs-simple-llm-workflow">Agentic vs. Simple LLM Workflow</h4> <p>An agent architecture would be more suitable when:</p> <ul> <li>LLM needs to plan or explore decision paths for what to do next</li> <li>Complex tool calling is required</li> <li>Control flow isn’t immediately apparent and may need to be decided by the LLM</li> </ul> <p>In this game workflow, the LLM didn’t need to determine program control flow. The game provided all necessary data for decision-making, eliminating the need for an agent-based approach. This realization highlighted a crucial design principle: choose the simplest workflow that effectively solves the problem.</p> <h4 id="key-lessons-learned">Key Lessons Learned</h4> <p>Simplicity Over Complexity: An elaborate agent architecture wasn’t necessary <em>for this particular use case</em>. A streamlined LLM-driven workflow proved more effective.</p> <p>Direct Computation: Handling calculations directly within the workflow maintained system efficiency.</p> <p>Reminder: As the design principle goes, “Keep It Simple, Stupid” (KISS), the simplest solution often works best. So, if you’re working on your own projects or systems, remember that sometimes less really is more. Apply this thinking in your own endeavors and don’t be afraid to reevaluate your approach as you go!</p> <h4 id="workflow-components">Workflow Components</h4> <p>The refined workflow included the following components:</p> <ol> <li> <p><strong>Financial Statement Management</strong>: The game maintains a comprehensive financial statement, tracking the player’s cash, income, expenses, assets, and liabilities. This allows the LLM to make informed recommendations based on the player’s current financial situation.</p> </li> <li> <p><strong>Opportunity Evaluation</strong>: When presented with an investment opportunity (for example, a deal or a shared market opportunity), the player’s details are passed to the LLM, which evaluates the opportunity and provides a recommendation on whether to buy or sell, along with the reasoning behind the decision.</p> </li> <li> <p><strong>Deal Size Recommendation</strong>: The LLM also provides guidance on the appropriate deal size based on the player’s financial position, helping them make informed decisions about the scale of their investments.</p> </li> <li> <p><strong>Charity</strong>: The workflow evaluates whether charity donations are possible using the player’s financial positions and decides if donating to charity is beneficial.</p> </li> <li> <p><strong>No-Deal Scenarios</strong>: The game also handles situations where the player encounters events like payday, downsizing, having a baby, Doodad cards (e.g., you’re buying a TV), or any other no-deal scenarios. The workflow calculates the payments and manages the player’s financial statement.</p> </li> <li> <p><strong>Borrowing/Repayment Decisions</strong>: The workflow includes logic for borrowing/repayment (from/to the bank at a high interest rate) decisions, with the LLM offering recommendations tailored to the player’s financial circumstances.</p> </li> </ol> <p>The project demonstrated the potential of LLMs in creating an interactive Cashflow game experience without the need for a complex agent architecture, highlighting thoughtful system design.</p> <p>If you’re interested in learning more about the implementation details, stay tuned for the second part of this post!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Introduction -- Unlocking the Power of LLMs for the Cashflow Game]]></summary></entry></feed>